{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow_text\n",
    "!git clone https://github.com/Jmkernes/PAR-Transformer-XL.git\n",
    "%cd PAR-Transformer-XL/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.info(\"\\n\\n~~~~~~~~ Importing Modules ~~~~~~~~\\n\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import DataManager\n",
    "from utils import print_bar, visualize_pi_weights\n",
    "from par_model import PARTransformerXL\n",
    "from par_model import create_lookahead_mask, positional_encoding\n",
    "\n",
    "tf.debugging.disable_check_numerics()\n",
    "\n",
    "def load_datasets(train, val, test):\n",
    "    \"\"\"Load the wikitext2 train, validation and test data\"\"\"\n",
    "    logging.info(f\"\\nLoading training data from: {train}\")\n",
    "    config = {'tfrecords_directory': train,'sp_model_prefix': 'wiki2_12k'}\n",
    "    train_dm = DataManager.initialize_from_tfrecord(config)\n",
    "\n",
    "    logging.info(f\"\\nLoading validation data from: {val}\")\n",
    "    config['tfrecords_directory'] = val\n",
    "    valid_dm = DataManager.initialize_from_tfrecord(config)\n",
    "\n",
    "    logging.info(f\"\\nLoading testing data from: {test}\\n\")\n",
    "    config['tfrecords_directory'] = test\n",
    "    test_dm = DataManager.initialize_from_tfrecord(config)\n",
    "\n",
    "    return train_dm, valid_dm, test_dm\n",
    "\n",
    "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, max_lr, decay_steps, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_lr = max_lr\n",
    "        self.decay_steps = decay_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = 3.1415927\n",
    "    def __call__(self, step):\n",
    "        linear = self.max_lr*(step/self.warmup_steps)\n",
    "        cosine = 0.5*self.max_lr*(1+tf.math.cos(self.pi*tf.math.maximum(step-self.warmup_steps, 0)/self.decay_steps))\n",
    "        return tf.math.minimum(linear, cosine)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training parameters: model, data, checkpointing, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing and tensorboard params\n",
    "model_name=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# data params\n",
    "train_directory='data/wikitext2_bsz32_seqlen32_tfrecords_train'\n",
    "valid_directory='data/wikitext2_bsz32_seqlen32_tfrecords_valid'\n",
    "test_directory='data/wikitext2_bsz32_seqlen32_tfrecords_test'\n",
    "\n",
    "# model params\n",
    "d_model=256\n",
    "num_heads=8\n",
    "d_ffn=1024\n",
    "num_layers=12\n",
    "mem_len=32\n",
    "dropout_rate=0.1\n",
    "cutoffs=[250, 2500]\n",
    "proj_factor=4\n",
    "proj_dims=[]\n",
    "straight_through=True\n",
    "\n",
    "# learning params\n",
    "warmup_steps=2000\n",
    "tau_start=2.0\n",
    "tau_end=0.2\n",
    "epochs=5\n",
    "tau_is_trainable=False\n",
    "opt_name='adam'\n",
    "max_lr=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set optimizer, tau. Automatically extract global constants from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of some flags logic beyond simple constraints.\n",
    "if d_model%num_heads:\n",
    "    raise ValueError('Number of heads must divide d_model')\n",
    "\n",
    "train_dm, valid_dm, test_dm = load_datasets(train_directory, valid_directory, test_directory)\n",
    "\n",
    "## Set global constants inferred from the training data.\n",
    "BATCH_SIZE = train_dm.batch_size\n",
    "SEQ_LEN = train_dm.seq_len\n",
    "VOCAB_SIZE = train_dm.tokenizer.vocab_size().numpy()\n",
    "DATASET_SIZE = train_dm.ds_size.numpy()\n",
    "MAX_POSITION = max(512, mem_len+SEQ_LEN)\n",
    "\n",
    "# Take care of additional constraints on inputs that needed the vocab size\n",
    "if any([z>=VOCAB_SIZE for z in cutoffs]) or len(set(cutoffs))!=len(cutoffs):\n",
    "    raise ValueError(\"Cutoffs must not exceed {VOCAB_SIZE} or contain duplicates.\")\n",
    "if cutoffs:\n",
    "    cutoffs.sort()\n",
    "    cutoffs.append(VOCAB_SIZE)\n",
    "\n",
    "### Define learning rate schedule and simulated annealing schedule for gumbel softmax temperature tau.\n",
    "logging.info(f\"\\n\\nInitializing {opt_name} optimizer with {warmup_steps} warmup steps.\")\n",
    "learning_rate = CosineSchedule(max_lr=max_lr, warmup_steps=warmup_steps,\n",
    "                               decay_steps=DATASET_SIZE*epochs-warmup_steps)\n",
    "optimizer = tf.keras.optimizers.get(opt_name)\n",
    "optimizer.learning_rate = learning_rate\n",
    "optimizer.clipvalue = 0.1\n",
    "\n",
    "if tau_is_trainable:\n",
    "    logging.info(f\"\\n\\nInitializing exponential tau decay: {tau_start}-->{tau_end}.\\n\")\n",
    "tau = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=tau_start,\n",
    "    decay_steps=DATASET_SIZE*epochs,\n",
    "    decay_rate=tau_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model architecture and print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "tf.keras.backend.clear_session()\n",
    "config = {\n",
    "    'd_model': d_model,\n",
    "    'num_heads': num_heads,\n",
    "    'max_position': MAX_POSITION,\n",
    "    'd_ffn': d_ffn,\n",
    "    'num_layers': num_layers,\n",
    "    'mem_len': mem_len,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'cutoffs': cutoffs,\n",
    "    'proj_factor': proj_factor,\n",
    "    'proj_dims': proj_dims,\n",
    "    'straight_through':True\n",
    "}\n",
    "logging.info(\"\\n\\nInitializing model...\")\n",
    "logging.info(\"Model parameters:\")\n",
    "logging.info(config)\n",
    "# pos_enc = positional_encoding(MAX_POSITION, d_model)\n",
    "# lookahead_mask = create_lookahead_mask(MAX_POSITION, MAX_POSITION)\n",
    "model = PARTransformerXL(**config)\n",
    "\n",
    "# Build model by feeding in sample training data\n",
    "x_temp, y_temp = next(iter(train_dm.get_inp_tar_pairs()))\n",
    "model(x_temp, None, labels=y_temp, training=False)\n",
    "\n",
    "# make tau untrainable\n",
    "if not tau_is_trainable:\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'tau'):\n",
    "            layer.tau = tf.cast(tf.constant(1.), tf.float32)\n",
    "            \n",
    "# print out model summary\n",
    "logging.info(\"\\nModel summary:\")\n",
    "logging.info(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define metrics. Define training step. Define evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "train_perp = tf.keras.metrics.Mean()\n",
    "valid_perp = tf.keras.metrics.Mean()\n",
    "\n",
    "logging.info(\"\\n\\nDefining training and evaluation steps...\")\n",
    "# Define the training and evaluation steps via tf functions\n",
    "@tf.function\n",
    "def train_step(inp, x_mems, labels, tau):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, mems = model(inp, x_mems=x_mems, labels=labels, training=True, tau=tau)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "#     grads = [tf.clip_by_norm(g, 0.1) for g in grads]\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_perp(tf.math.exp(loss))\n",
    "    return mems\n",
    "\n",
    "@tf.function\n",
    "def evaluation_step(x, x_mems, labels, tau):\n",
    "    loss, mems = model(x, x_mems=x_mems, labels=labels, tau=tau, training=False)\n",
    "    valid_loss(loss)\n",
    "    valid_perp(tf.math.exp(loss))\n",
    "    return mems\n",
    "\n",
    "def evaluation(dataset, tau):\n",
    "    x_mems = None\n",
    "    for x, lbl in dataset:\n",
    "        x_mems = evaluation_step(x, x_mems, lbl, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard logging, set up checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TensorBoard\n",
    "logging.info(\"\\n\\nInitializing TensorBoard...\")\n",
    "train_log_dir = './logs/' + model_name + '/train'\n",
    "test_log_dir = './logs/' + model_name + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "# load tensorboard using in-notebook command\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "# Configure datasets for training\n",
    "logging.info(\"\\n\\nConfiguring datasets for training. Caching, prefetching...\")\n",
    "glob_step = tf.Variable(0, dtype=tf.int64) # This will break tf.summary if we use int32\n",
    "train_ds = train_dm.get_inp_tar_pairs().cache().prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_dm.get_inp_tar_pairs().prefetch(tf.data.AUTOTUNE)\n",
    "iterator=iter(train_ds)\n",
    "\n",
    "# Set up checkpointing to periodically save the model every epoch\n",
    "checkpoint_path = \"./checkpoints/train/\"+model_name\n",
    "logging.info(f\"\\n\\nInitializing checkpoints. Models will be saved to {checkpoint_path}\")\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    glob_step=glob_step,\n",
    "    iterator=iterator\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    try:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        logging.info('Latest checkpoint restored!!')\n",
    "    except:\n",
    "        logging.warning(\"Model may have changed, could not restore checkpoint.\")\n",
    "    with open(checkpoint_path+'/config', 'w') as file:\n",
    "        file.write(json.dumps(config))\n",
    "    logging.info(f\"Writing model configuration to {checkpoint_path+'/config'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the actual training loop!\n",
    "logging.info(\"\\n\\n~~~~~~~~~~ Beginning training ~~~~~~~~~~\")\n",
    "for epoch in range(epochs):\n",
    "\n",
    "#     logging.info('-'*10+f' Epoch {epoch+1}/{epochs} '+'-'*10)\n",
    "    print('-'*10+f' Epoch {epoch+1}/{epochs} '+'-'*10)\n",
    "    start = time.time()\n",
    "    for x in [train_loss, valid_loss, train_perp, valid_perp]:\n",
    "        x.reset_states()\n",
    "    mems = None\n",
    "\n",
    "    for step, (inp, lbl) in enumerate(train_ds):\n",
    "\n",
    "        mems = train_step(inp, mems, lbl, tau(glob_step))\n",
    "\n",
    "        diff = (time.time()-start)/(step+1)\n",
    "        print_bar(step, DATASET_SIZE, diff, train_loss.result().numpy())\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('train_loss', train_loss.result(), step=glob_step)\n",
    "            tf.summary.scalar('train_perp', train_perp.result(), step=glob_step)\n",
    "            tf.summary.scalar('tau', tau(glob_step), step=glob_step)\n",
    "            tf.summary.scalar('lr', learning_rate(tf.cast(glob_step, tf.float32)),\n",
    "                              step=glob_step)\n",
    "        glob_step.assign_add(1)\n",
    "        \n",
    "    if not (step+1)%1000:\n",
    "        print(\"Global step:\", glob_step.numpy())\n",
    "        visualize_pi_weights(model)\n",
    "        plt.show()\n",
    "\n",
    "    evaluation(valid_ds, tau(glob_step))\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('valid_loss', valid_loss.result(), step=glob_step)\n",
    "        tf.summary.scalar('valid_perp', valid_perp.result(), step=glob_step)\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    logging.info(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "    \n",
    "    tot_time = time.time()-absolute_start\n",
    "    minutes = tot_time//60\n",
    "    seconds = tot_time%60\n",
    "    logging.info('*'*100)\n",
    "    logging.info(\"\\n\\nTRAINING COMPLETE.\\n\\n\")\n",
    "    logging.info('*'*100)\n",
    "    logging.info(f\"\\n\\nTotal time: {minutes:.02d}min. {seconds:.02d}sec.\\n\\n\")\n",
    "    try:\n",
    "        os.mkdir('saved_models')\n",
    "    except:\n",
    "        pass\n",
    "    logging.info(f'Saving final model to {'saved_models/'+FLAGS.model_name}')\n",
    "    model.save('saved_models/'+FLAGS.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize weights, model inference, loading saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pi_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below will work to load in from checkpoint! You have to \n",
    "\n",
    "1) recreate an identical model with the same architecture\n",
    "\n",
    "2) create a checkpoint object with parameter model=model. The key here was decided when the first model was checkpointed, i.e., that the model should always be called model.\n",
    "\n",
    "3) restore the checkpoint object with a checkpoint path ckpt.restore(PATH). This will automatically change the value of model globally, i.e. ckpt doesn't keep a copy of model, it keeps a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/train/dmodel256_dffn1024_blocks12/ckpt-1'"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_from_checkpoint(ckpt_path):\n",
    "    with open(ckpt_path+'/config.json', 'r') as file:\n",
    "        config = json.loads(file.readline())\n",
    "    model = PARTransformerXL(**config)\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_path, 5)\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_savedmodel(path):\n",
    "    return tf.keras.models.load_model(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
